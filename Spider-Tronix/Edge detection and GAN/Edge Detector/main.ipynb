{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I Used layers 1,2,8,9,10 alone from the layers.py !!! so only the corresponding tags in my weights, gradients, classes and filters are being used\n",
    "Functions: forwardProp, backProp, backPool, Backconvolution\n",
    "global variables:  m-number of training samples, init_size-the number of rows/columns for the initial images, filters for conv layers and weights for fc layers\n",
    "epsilon is used to prevent overflowing of my sigmoid(for eg -1.5 * 10^8 causes overflowing)input[] - to store input images , output[] to store output images, learning rate, epochs, accuracy, cost[] - cost function.\n",
    "'''\n",
    "''' my predictions always converged to 0.5. i tried many different ways to execute my algorithm but still i get the same result. i wasnt able to figure out my error'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import layers\n",
    "import costfunction\n",
    "import gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "init_size=8\n",
    "\n",
    "input = np.zeros((m,init_size,init_size), np.float64)\n",
    "output = np.zeros((m,1), np.float64)\n",
    "permute = np.random.permutation(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "lamda = 0\n",
    "lr =0.1\n",
    "epsilon = 10**(-4)\n",
    "filter1 = np.random.randn(2,1,3,3)*2*epsilon - epsilon\n",
    "filter2 = np.random.randn(4,2,3,3)*2*epsilon - epsilon\n",
    "THETA8 = np.random.randn(4,5)*2*epsilon - epsilon\n",
    "THETA9 = np.random.randn(2,5)*2*epsilon - epsilon\n",
    "THETA10 = np.random.randn(1,3)*2*epsilon - epsilon\n",
    "\n",
    "cost = np.zeros((epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(Filter1, Filter2,Theta8, Theta9, Theta10):\n",
    "    a1 = layers.layer1(input.reshape((m,1,init_size,init_size)),Filter1)\n",
    "    a2 = layers.layer2(a1.final,Filter2)\n",
    "    a8 = layers.layer8(a2.final.reshape((m, np.size(a2.final,1))),Theta8)\n",
    "    a9 = layers.layer9(a8.final, Theta9)\n",
    "    a10 = layers.layer10(a9.final,Theta10)\n",
    "    loss = (costfunction.sigmoidClassifier(a10.final, output) + 0.5*lamda*(np.sum(np.square(Theta8))+np.sum(np.square(Theta9))+np.sum(np.square(Theta10))))/m\n",
    "    return loss,a1,a2,a8,a9,a10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''normal backProp, here we call both back convolution and backpool, i tried to train the whole network for some epochs and my fcs alone for more epochs since training conv layers required much time'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(a1,a2,a8,a9,a10,output,iter):\n",
    "\n",
    "    #del10 = (gradient.svmGradient1(a10.final, output) + gradient.svmGradient2(a10.final, output)).transpose()\n",
    "    del10 = (a10.final - output).transpose()\n",
    "    del9_ = np.multiply(np.dot(a10.THETA.transpose(), del10), gradient.sigmoidGradient(a10.input.transpose()))\n",
    "    del9 = del9_[1:,:]\n",
    "    del8_ = np.multiply(np.dot(a9.THETA.transpose(), del9), gradient.sigmoidGradient(a9.input.transpose()))\n",
    "    del8 = del8_[1:,:]\n",
    "    #del7_ = np.multiply(np.dot(a8.THETA.transpose(), del8), gradient.leakyReluGradient(a8.biased.transpose()))\n",
    "    #del7 = del7_[1:,:]\n",
    "    #del6_ = np.multiply(np.dot(a8.THETA.transpose(), del8), gradient.sigmoidGradient(a8.biased.transpose()))\n",
    "    #del6 = del6_[1:,:]\n",
    "    grad_10 = (np.dot(del10, a10.input) + lamda*(np.hstack((np.zeros((np.size(a10.THETA, 0), 1)), a10.THETA[:,1:]))))/m\n",
    "    grad_9 = (np.dot(del9, a9.input) + lamda*(np.hstack((np.zeros((np.size(a9.THETA, 0), 1)), a9.THETA[:,1:]))))/m\n",
    "    grad_8 = (np.dot(del8, a8.input)+ lamda*(np.hstack((np.ones((np.size(a8.THETA, 0), 1)), a8.THETA[:,1:]))))/m\n",
    "    #grad_7 = np.dot(del7, a7.biased)/m\n",
    "    #grad_6 = np.dot(del6, a6.biased)/m\n",
    "\n",
    "\n",
    "    if(iter >= 25):\n",
    "        return grad_8,grad_9, grad_10\n",
    "    #del5_ = np.multiply(np.dot(a6.THETA.transpose(), del6), gradient.leakyReluGradient(a6.biased.transpose()))\n",
    "    #del5 = del5_[1:,:]\n",
    "    #error_a5 = backPool(a5, del5.reshape((1000,10,1,1)),2)\n",
    "    else:\n",
    "        del2_ = np.multiply(np.dot(a8.THETA.transpose(), del8), gradient.sigmoidGradient(a8.input.transpose()))/m\n",
    "        del2 = del2_[1:,:]\n",
    "\n",
    "        error_a2 = backPool(a2, del2.reshape((m,np.size(a2.final, 1),1,1)),2)\n",
    "        del1 = Backconvolution(error_a2, a2)\n",
    "        #print(np.shape(del4))\n",
    "        error_a1 = backPool(a1, del1, 3)\n",
    "\n",
    "        #del2 = Backconvolution(error_a3, a3)\n",
    "        #error_a2 = backPool(a2,del2,3)\n",
    "\n",
    "        #del1 = Backconvolution(error_a2, a2)\n",
    "        #error_a1 = backPool(a1,del1,3)\n",
    "        del0 = Backconvolution(error_a1, a1)\n",
    "    #error_a1 = backPool(a1,del1,3)\n",
    "    #del0 = Backconvolution(error_a1,a1)\n",
    "\n",
    "        return a1.err_filter,a2.err_filter,grad_8, grad_9, grad_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this function uses the mask created in the layers.py to reproduce the matrix of the size before pooling, we multiply the 1's in the matrix to the appropriate errors. remaining elements is 0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPool(a, error, poolSize):\n",
    "    tuples = list(a.indices.items())\n",
    "\n",
    "    #err_unpool= np.zeros((np.size(matrix,0), np.size(matrix,1), np.size(matrix,2)+size-1, np.size(matrix,3)+size-1))\n",
    "    err_unpool = a.mask\n",
    "    for i in range(0,m):\n",
    "        for l in range(0,np.size(error, 1)):\n",
    "            for j in range(0,np.size(error, 2)):\n",
    "                for k in range(0,np.size(error, 3)):\n",
    "                    err_unpool[i,l,j:j+poolSize, k:k+poolSize] *= error[i,l,j,k]\n",
    "    '''for channel in tuples:\n",
    "        channel_dim = channel[0]\n",
    "        for i in range(0,m):\n",
    "            for j in range(0,np.size(error,2)):\n",
    "                for k in range(0,np.size(error,3)):\n",
    "                    #err_unpool[i, channel_dim, channel[i*imSize*imSize + j*imSize + k][0], channel[i*imSize*imSize + j*imSize + k][1]] = matrix[i,channel_dim,j,k]\n",
    "\n",
    "                    err_unpool[i, channel_dim, channel[1][i*a.imSize*a.imSize + j*a.imSize + k][0], channel[1][i*a.imSize*a.imSize + j*a.imSize + k][1]] = error[i,channel_dim,j,k]'''\n",
    "    #print(err_unpool[:1,:1,:,:])\n",
    "    return err_unpool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find error in filter and input. first the derivative wrt activation function is taken.\n",
    "def Backconvolution(error,a):\n",
    "    #back_relu = np.multiply(error, gradient.reluGradient(a.activated-1))   #subtract bias units\n",
    "\n",
    "    back_relu = np.multiply(error, gradient.leakyReluGradient(a.activated))\n",
    "\n",
    "    err_filter = np.zeros(np.shape(a.weights))\n",
    "    err_input = np.zeros(np.shape(a.padding))\n",
    "    cross_180 = np.zeros(np.shape(a.weights))\n",
    "    for i in range(0,np.size(err_filter, 0)):                                                #convolution between dow l / dow output and the input image\n",
    "        for j in range(0,np.size(err_filter, 1)):\n",
    "            cross_180[i,j,:,:] = np.flipud(np.fliplr(a.weights[i,j,:,:]))                    # we flip the filter to perform full convolution afterwards\n",
    "            for k in range(0,np.size(err_filter, 2)):\n",
    "                for l in range(0,np.size(err_filter, 3)):\n",
    "                    err_filter[i,j,k,l] += np.sum(np.multiply(back_relu[:,i,:,:], a.padding[:,j,k:k+np.size(back_relu, 2), l:l+np.size(back_relu, 3)]))\n",
    "                    #i dint use padding to minimize the number of layers. but i used a.padding everywhere before so.. this is unpadded normal matrix\n",
    "\n",
    "    p = (np.size(a.weights, 2) > np.size(back_relu, 2))\n",
    "    q = (np.size(a.weights, 3) > np.size(back_relu, 3))  #since its a square matrix we dont need this\n",
    "    '''idea: this can be splitted into three regions, 0-min length Rows/columns(filter, error in output); between length of rows/columns of the two, greather than the length of the rows of the two\n",
    "    so during the first stage, we can index the filter from last(ie using -1), in the second one the full column of the filter will be convolved, third stage is similar to that of the first. since it is a square matrix, operating this on columns is of the same syntax as that of the rows'''\n",
    "    for i in range(0,m):\n",
    "        for k in range(0,np.size(cross_180, 1)):\n",
    "            for l in range(0,np.size(cross_180, 2)+np.size(back_relu, 2) - 1):\n",
    "                for h in range(0, np.size(cross_180, 3)+ np.size(back_relu, 3) - 1):\n",
    "\n",
    "                    if(l <= min(np.size(cross_180, 2), np.size(back_relu, 2))-1):\n",
    "                        if(p==0):\n",
    "                            if(h<=min(np.size(cross_180, 3), np.size(back_relu, 3))-1):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:l+1,:h+1], cross_180[:,k,-1-l : , -1-h :] ))\n",
    "                            elif(h in range(np.size(cross_180, 3), np.size(back_relu, 3))):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:l+1,h-np.size(cross_180, 3)+1:h+1], cross_180[:,k,-1-l:,:]))\n",
    "                            else:\n",
    "                                #print(\"i\",i,\"k\",k,\"l\",l,\"h\",h,np.size(back_relu,3),np.size(cross_180,3))\n",
    "                                #print(np.shape(back_relu[i,:,:l+1,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):]), np.shape(cross_180[:,k,-1-l:, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:l+1,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):], cross_180[:,k,-1-l:, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "                        else:\n",
    "                            if(h<=min(np.size(cross_180, 3), np.size(back_relu, 3))-1):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:l+1,:h+1], cross_180[:,k,-1-l : , -1-h :] ))\n",
    "                            elif(h in range(np.size(cross_180, 3), np.size(back_relu, 3))):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:l+1,:], cross_180[:,k,-1-l:,-1-h : np.size(back_relu, 3)-1-h]))\n",
    "                            else:\n",
    "                                #print(np.shape(back_relu[i,:,:l+1,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):]), np.shape(cross_180[:,k,-1-l:, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:l+1,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):], cross_180[:,k,-1-l:, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "\n",
    "                    elif(l in range(np.size(cross_180, 2), np.size(back_relu, 2))):\n",
    "                        if(p==0):\n",
    "                            if(h<=min(np.size(cross_180, 3), np.size(back_relu, 3))-1):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l-np.size(cross_180, 2)+1:l+1,:h+1], cross_180[:,k,:, -1-h :] ))\n",
    "                            elif(h in range(np.size(cross_180, 3), np.size(back_relu, 3))):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l-np.size(cross_180, 2)+1:l+1, h-np.size(cross_180, 3)+1:h+1], cross_180[:,k,:,:]))\n",
    "                            else:\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l-np.size(cross_180, 2)+1:l+1, h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):], cross_180[:,k,:, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "                        else:\n",
    "                            if(h<=min(np.size(cross_180, 3), np.size(back_relu, 3))-1):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:,:h+1], cross_180[:,k,-1-l : np.size(back_relu, 2)-1-l , -1-h :] ))\n",
    "                            elif(h in range(np.size(cross_180, 3), np.size(back_relu, 3))):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:,:], cross_180[:,k,-1-l : np.size(back_relu, 2)-1-l, -1-h : np.size(back_relu, 3)-1-h]))\n",
    "                            else:\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,:,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):], cross_180[:,k,-1-l : np.size(back_relu, 2)-1-l, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "\n",
    "                    else:\n",
    "                        if(p==0):\n",
    "                            if(h<=min(np.size(cross_180, 3), np.size(back_relu, 3))-1):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l+1 - np.size(back_relu, 2) - np.size(cross_180, 2):,:h+1], cross_180[:,k,:np.size(cross_180, 2)+np.size(back_relu, 2)-l-1 , -1-h :] ))\n",
    "                            elif(h in range(np.size(cross_180, 3), np.size(back_relu, 3))):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l+1 - np.size(back_relu, 2) - np.size(cross_180, 2):,h-np.size(cross_180, 3)+1:h+1], cross_180[:,k,:np.size(cross_180, 2)+np.size(back_relu, 2)-l-1,:]))\n",
    "                            else:\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l+1 - np.size(back_relu, 2) - np.size(cross_180, 2):,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):], cross_180[:,k, :np.size(cross_180, 2)+np.size(back_relu, 2)-l-1 , :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "                        else:\n",
    "                            if(h<=min(np.size(cross_180, 3), np.size(back_relu, 3))-1):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l+1 - np.size(back_relu, 2) - np.size(cross_180, 2):,:h+1], cross_180[:,k,:np.size(cross_180, 2)+np.size(back_relu, 2)-l-1 , -1-h :] ))\n",
    "                            elif(h in range(np.size(cross_180, 3), np.size(back_relu, 3))):\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l+1 - np.size(back_relu, 2) - np.size(cross_180, 2): ,:], cross_180[:,k,:np.size(cross_180, 2)+np.size(back_relu, 2)-l-1,-1-h : np.size(back_relu, 3)-1-h]))\n",
    "                            else:\n",
    "                                err_input[i,k,l,h] = np.sum(np.multiply(back_relu[i,:,l+1 - np.size(back_relu, 2) - np.size(cross_180, 2):,h+1 - np.size(back_relu, 3) - np.size(cross_180, 3):], cross_180[:,k,:np.size(cross_180, 2)+np.size(back_relu, 2)-l-1, :np.size(cross_180, 3)+np.size(back_relu, 3)-h-1]))\n",
    "\n",
    "    a.err_filter = err_filter\n",
    "    err_input = err_input[:,:,1:-1,:]\n",
    "    err_input = err_input[:,:,:,1:-1]\n",
    "    a.err_input = err_input\n",
    "\n",
    "    return err_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in permute:\n",
    "    img = cv2.imread(f\"E:\\\\Prem\\\\cnn\\\\dataset\\\\{i}.JPG\", 0)\n",
    "    input[i] = img\n",
    "    output[i] = 0 if i<500 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 25):\n",
    "    cost[i],a1,a2,a8,a9,a10 = forwardProp(filter1,filter2,THETA8, THETA9,THETA10)\n",
    "    err_filter1,err_filter2,grad_8, grad_9, grad_10 = backProp(a1,a2,a8,a9,a10,output,i)\n",
    "    filter1 -= lr*err_filter1\n",
    "    filter2 -= lr*err_filter2\n",
    "    #filter3 -= lr*err_filter3\n",
    "    #filter4 -= lr*err_filter4\n",
    "    #filter5 -= lr*err_filter5\n",
    "    #THETA6 -= lr*grad_6\n",
    "    #THETA7 -= lr*grad_7\n",
    "    THETA8 =THETA8 -  lr*grad_8\n",
    "\n",
    "    THETA9 =THETA9 - lr*grad_9\n",
    "    THETA10 =THETA10 -  lr*grad_10\n",
    "\n",
    "for j in range(25,epochs):\n",
    "    cost[j],a1,a2,a8,a9,a10 = forwardProp(filter1,filter2,THETA8,THETA9,THETA10)\n",
    "    grad_8, grad_9, grad_10 = backProp(a1,a2,a8,a9,a10,output,j)\n",
    "    #THETA6 -= lr*grad_6\n",
    "    #THETA7 -= lr*grad_7\n",
    "    THETA8 = THETA8 - lr*grad_8\n",
    "    THETA9 = THETA9 -  lr*grad_9\n",
    "    THETA10 =THETA10 - lr*grad_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight8 = np.reshape(THETA8, (np.size(THETA8), 1))\n",
    "weight9 = np.reshape(THETA9, (np.size(THETA9), 1))\n",
    "weight10 = np.reshape(THETA10, (np.size(THETA10), 1))\n",
    "filter_1 = filter1.reshape((np.size(filter1), 1))\n",
    "filter_2 = filter2.reshape((np.size(filter2), 1))\n",
    "mean_1 = a1.mean.reshape((np.size(a1.mean), 1))\n",
    "std_1 = a1.std.reshape((np.size(a1.std), 1))\n",
    "mean_2 = a2.mean.reshape((np.size(a2.mean), 1))\n",
    "std_2 = a2.std.reshape((np.size(a2.std), 1))\n",
    "with open(r'E:\\Prem\\cnn\\weights.npy', 'w') as f:\n",
    "    np.save(r'E:\\Prem\\cnn\\weights.npy', np.vstack((weight8,weight9,weight10)))\n",
    "with open(r'E:\\Prem\\cnn\\filters.npy', 'w') as f:\n",
    "    np.save(r'E:\\Prem\\cnn\\filters.npy', np.vstack((filter_1, filter_2)))\n",
    "with open(r'E:\\Prem\\cnn\\norm.npy', 'w') as f:\n",
    "    np.save(r'E:\\Prem\\cnn\\norm.npy', np.vstack((mean_1, std_1, mean_2, std_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([k for k in range(0,epochs)], cost)\n",
    "plt.show()\n",
    "accuracy = np.sum(np.fabs(output-a10.final))/m\n",
    "print(accuracy)\n",
    "print(cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
