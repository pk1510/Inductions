{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import activations\n",
    "import layers\n",
    "import train\n",
    "\n",
    "''' the pounded comments were used to determine lambda by splitting my data into train and test set. I found lambda = 0 gave the least error'''\n",
    "'''i tried lr = 0.03 and got a smooth curve but it stayed in a local minimum(error = 0.14) for more than 700 iterations so i used\n",
    "lr = 0.1 or 0.3 and reduced my number of iterations. and now the minimum is around 0.00014 although the curve is not smooth throughout.I\n",
    "have attached those images'''\n",
    "\n",
    "\n",
    "df = pd.read_excel(r'E:\\Prem\\python\\spyder\\data.xlsx')\n",
    "df_ = df\n",
    "cont = list(df[\"continent\"].unique())\n",
    "cont.remove(cont[-1])                                  # to remove Nan\n",
    "loc = list(df[\"location\"].unique())\n",
    "loc.remove('World')\n",
    "loc.remove('International')\n",
    "df = df.dropna(subset = ['continent', 'new_cases', 'new_deaths'])    # drop the rows if it has Nan in any of the 3 columns\n",
    "copy = df\n",
    "#'''to preserve the original dataframe'''\n",
    "for continent in cont:\n",
    "    df.loc[df.continent == continent, \"continent\"] = float(cont.index(continent) + 1)    #'''Asia=1, etc'''\n",
    "for location in loc:\n",
    "    df.loc[df.location == location, \"location\"] = float(loc.index(location) + 1)\n",
    "    arr=(df.loc[df.location == loc.index(location) + 1, [\"date\", \"new_cases\", \"new_deaths\", \"continent\", \"location\"]]).to_numpy()\n",
    "    arr1 = arr[:,:3]\n",
    "    arr1[1:,1:3]=arr[:-1,1:3]\n",
    "                                                  #''' the date of nth row in a location shld match with new cases and death of n-1th location'''\n",
    "    arr1 = arr1[1:,:]\n",
    "    rows, columns = np.indices((np.size(arr1,0),1))\n",
    "    arr1[rows, columns] = rows + 2\n",
    "    arr1 = np.insert(arr1, 1, arr1[:,0]**2, axis=1)      # ''' i plotted new cases and deaths against number of days and found it to be a quadrtatic dependence'''\n",
    "    if(loc.index(location) == 0):\n",
    "        previous = arr1                                          #'''parsing location by location and preparing the input for layer 2'''\n",
    "        #previous_train = arr1[:math.floor(0.8 * np.size(arr1,0)), :]\n",
    "        #previous_test = arr1[math.floor(0.8 * np.size(arr1,0)):,:]\n",
    "\n",
    "        Y=arr[1:,1:3]                                             #'''parsing location by location and preparing the output layer. '''\n",
    "        #Y_train = arr[1:math.floor(0.8 * np.size(arr1,0))+1, 1:3]\n",
    "        #Y_test = arr[math.floor(0.8 * np.size(arr1,0))+1:, 1:3]\n",
    "\n",
    "        D=arr[1:,3:]                                               #'''parsing location by location and preparing the input for layer 1'''\n",
    "\n",
    "        #D_train = arr[1:math.floor(0.8 * np.size(arr1,0))+1, 3:]\n",
    "        #D_test = arr[math.floor(0.8 * np.size(arr1,0))+1:, 3:]\n",
    "\n",
    "    else:\n",
    "        #previous_train = np.vstack((previous_train, arr1[:math.floor(0.8 * np.size(arr1,0)), :]))\n",
    "        #previous_test = np.vstack((previous_test, arr1[math.floor(0.8 * np.size(arr1,0)):,:]))\n",
    "        previous = np.vstack((previous, arr1))    # second layer input\n",
    "\n",
    "        #Y_train = np.vstack((Y_train, arr[1:math.floor(0.8 * np.size(arr1,0))+1, 1:3] ))\n",
    "        #Y_test = np.vstack((Y_test, arr[math.floor(0.8 * np.size(arr1,0))+1:, 1:3]))\n",
    "        Y = np.vstack((Y,arr[1:,1:3]))\n",
    "\n",
    "        #D_train = np.vstack((D_train, arr[1:math.floor(0.8 * np.size(arr1,0))+1, 3:] ))\n",
    "        #D_test = np.vstack((D_test, arr[math.floor(0.8 * np.size(arr1,0))+1:, 3:]))\n",
    "        D = np.vstack((D,arr[1:,3:]))                                                 #''' first row of each location should be omitted because it doesnt have previous cases and deaths'''\n",
    "#O = np.ones((np.size(D,0), 1))\n",
    "epsilon = 10**(-8)\n",
    "Theta1 = (np.random.rand(5,3)) * (2*epsilon) - epsilon   #''' 5x3 matrix whose absolute values are less than epsilon'''\n",
    "Theta2 = (np.random.rand(7,10)) * (2*epsilon) - epsilon\n",
    "Theta3 = (np.random.rand(5,8)) * (2*epsilon) - epsilon\n",
    "Theta4 = (np.random.rand(2,6)) * (2*epsilon) - epsilon               #''' random initialization of weights'''\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "D_Normed = layers.layer1(D)\n",
    "\n",
    "#D_Normed, meanD, stdD = featureNormalize.fNorm(D_train)\n",
    "#D_Normed_test, meanDtest, stdDtest = featureNormalize.fNorm(D_test)\n",
    "\n",
    "previous_Normed, meanPre, stdPre = train.fNorm(previous)\n",
    "#previous_Normed, meanPre, stdPre = featureNormalize.fNorm(previous_train)\n",
    "#previous_Normed_test, meanPretest, stdPretest = featureNormalize.fNorm(previous_test)\n",
    "\n",
    "Y_Normed=layers.layer5(Y)      #since input to second layer is normed lets norm this also\n",
    "#Y_Normed, meanY, stdY = featureNormalize.fNorm(Y_train)\n",
    "#Y_Normed_test, meanYtest, stdYtest = featureNormalize.fNorm(Y_test)\n",
    "num_iters = 500.0\n",
    "#Lambda = [0, 0.02 ** x for x in range(1,11)]\n",
    "Lambda=0\n",
    "lr = 0.03\n",
    "J_hist = np.arange(num_iters)                                # ''' to plot it against num of iterations'''\n",
    "#J_test_hist = []\n",
    "#hypo_hist = np.zeros((27150, 2, int(num_iters)))\n",
    "hypo_hist = np.zeros((np.size(Y_Normed.z, 0), 2, int(num_iters)))\n",
    "\n",
    "\n",
    "\n",
    "#for Lambda_ in Lambda:   '''to check which lambda gives the smallest error'''\n",
    "\n",
    "    #Theta1 = (np.random.rand(5,3)) * (2*epsilon) - epsilon\n",
    "    #Theta2 = (np.random.rand(7,10)) * (2*epsilon) - epsilon\n",
    "    #Theta3 = (np.random.rand(5,8)) * (2*epsilon) - epsilon\n",
    "    #Theta4 = (np.random.rand(2,6)) * (2*epsilon) - epsilon      '''re initialize theta for every lambda_\n",
    "\n",
    "for i in range(0,int(num_iters)):           # '''gradient descent'''\n",
    "    #hypo_hist[:,:,i], J_hist[i], des1, des2, des3, des4, meanz1, stdz1, meanz2, stdz2, meanz3, stdz3  = hypothesis.hypothesis(matrix_lay1, previous_Normed, Y_Normed, Theta1, Theta2, Theta3, Theta4, num_iters, Lambda)\n",
    "    hypo_hist[:,:,i], J_hist[i], des1, des2, des3, des4, meanz1, stdz1, meanz2, stdz2, meanz3, stdz3  = train.forBackprop(D_Normed.z, previous_Normed, Y_Normed.z, Theta1, Theta2, Theta3, Theta4, Lambda)\n",
    "    Theta1[:,1:] = Theta1[:,1:] - lr*des1\n",
    "    Theta2[:,1:] = Theta2[:,1:] - lr*des2\n",
    "    Theta3[:,1:] = Theta3[:,1:] - lr*des3\n",
    "    Theta4[:,1:] = Theta4[:,1:] - lr*des4\n",
    "\n",
    "#J_test_hist.append(hypothesis.costTest(np.hstack((O[:np.size(D_Normed_test, 0), :], D_Normed_test)), previous_Normed_test, Y_Normed_test, Theta1, Theta2, Theta3, Theta4))\n",
    "\n",
    "weight1 = Theta1.reshape((np.size(Theta1), 1))\n",
    "weight2 = Theta2.reshape((np.size(Theta2), 1))\n",
    "weight3 = Theta3.reshape((np.size(Theta3), 1))\n",
    "weight4 = Theta4.reshape((np.size(Theta4), 1))\n",
    "with open(r'E:\\Prem\\python\\spyder\\weights.npy', 'w') as f:\n",
    "    np.save(r'E:\\Prem\\python\\spyder\\weights.npy', np.vstack((weight1, weight2, weight3, weight4)))\n",
    "with open(r'E:\\Prem\\python\\spyder\\norm.npy', 'w') as f:\n",
    "    np.save(r'E:\\Prem\\python\\spyder\\norm.npy', np.hstack((D_Normed.mean, D_Normed.std, meanPre, stdPre, meanz1, stdz1, meanz2, stdz2, meanz3, stdz3, Y_Normed.mean, Y_Normed.std)))\n",
    "\n",
    "#print(J_test_hist)\n",
    "#plt.plot(list(range(0, len(Lambda) )), J_test_hist)\n",
    "#plt.title(lr)\n",
    "#plt.show()\n",
    "print((np.multiply(hypo_hist[:,:,-1], Y_Normed.std) + Y_Normed.mean ))\n",
    "print(Y_Normed.mean, Y_Normed.std)\n",
    "print(hypo_hist[:20,:,-1])\n",
    "#dataframe = pd.DataFrame(np.hstack((Y, (np.multiply(hypo_hist[:,:,-1], stdY) + meanY ))))\n",
    "#writer = pd.ExcelWriter(r'E:\\Prem\\python\\spyder\\train0.xlsx', engine='xlsxwriter')\n",
    "#dataframe.to_excel(writer)\n",
    "#writer.save()\n",
    "\n",
    "plt.plot(list(range(1, int(num_iters) + 1)), list(J_hist))\n",
    "plt.title(lr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_weights\n",
    "import numpy as np\n",
    "import math\n",
    "import activations\n",
    "\n",
    "cont_input = input(str(\"enter the continent\"))\n",
    "cont_input_float = float(cont.index(cont_input) + 1)\n",
    "loc_input = input(str(\"enter the country\"))\n",
    "loc_input_float = float(loc.index(loc_input) + 1)\n",
    "row = (df.loc[df.location == loc_input_float, [\"date\", \"new_cases\", \"new_deaths\"]]).to_numpy()\n",
    "\n",
    "day_input = input(str(\"enter the date\"))\n",
    "date_format = \"%d-%m-%Y\"\n",
    "a = datetime.strptime(day_input, date_format)\n",
    "\n",
    "delta = a - row[-1,0]\n",
    "\n",
    "\n",
    "\n",
    "days_needed = np.arange(np.size(row, 0) + 2, np.size(row,0) + 2 + delta.days, dtype=np.float32)\n",
    "\n",
    "A = np.array([cont_input_float, loc_input_float], dtype=np.float32)\n",
    "\n",
    "A = np.divide(A - D_Normed.mean, D_Normed.std)\n",
    "A = np.insert(A,0,1,axis=0)\n",
    "\n",
    "prev_case = row[-1,1]\n",
    "prev_deaths = row[-1,2]\n",
    "prev_case = (prev_case-load_weights.meanY[0])/load_weights.stdY[0]\n",
    "prev_deaths = (prev_deaths-load_weights.meanY[1])/load_weights.stdY[1]\n",
    "\n",
    "\n",
    "for i in days_needed:\n",
    "    \n",
    "    calc1 = activations.activation(A, load_weights.Theta1)\n",
    "\n",
    "    calc1_ = np.hstack((calc1, np.array([i, i**2, prev_case, prev_deaths]) ))\n",
    "\n",
    "    calc1Normed = np.divide(calc1_ - load_weights.meanz1 , load_weights.stdz1)\n",
    "\n",
    "    calc1_Normed = np.insert(calc1Normed,0,1,axis=0)\n",
    "\n",
    "    calc2 = activations.activation(calc1_Normed, load_weights.Theta2)\n",
    "    calc2Normed = np.divide(calc2 - load_weights.meanz2, load_weights.stdz2)\n",
    "    calc2_Normed = np.insert(calc2Normed,0,1,axis=0)\n",
    "\n",
    "    calc3 = activations.activation(calc2_Normed, load_weights.Theta3)\n",
    "    calc3Normed = np.divide(calc3 - load_weights.meanz3, load_weights.stdz3)\n",
    "    calc3_Normed = np.insert(calc3Normed,0,1,axis=0)\n",
    "\n",
    "    calc4 = activations.activation(calc3_Normed, load_weights.Theta4)\n",
    "    \n",
    "    prev_case, prev_deaths = calc4\n",
    "                                                     #''' we shld give normalized input again if we unnormalize here. so we can leave it as it is'''\n",
    "pred_cases, pred_deaths = np.multiply(np.floor(np.fabs(calc4.astype(np.float64), dtype=np.float64)), load_weights.stdY) + load_weights.meanY #unnorming\n",
    "print(\"the predicated new cases and deaths is \", pred_cases, pred_deaths)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
